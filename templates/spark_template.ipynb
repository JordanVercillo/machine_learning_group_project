{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session running in VS Code!\n",
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql.functions import col, lpad\n",
    "\n",
    "import os\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--driver-memory 8g pyspark-shell\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VSCodeTest\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark session running in VS Code!\")\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lpad\n",
    "\n",
    "# Load the datasets\n",
    "diamonds = (spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"C:/Users/jverc/Downloads/diamonds.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.createOrReplaceTempView(\"diamonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+---------+-----+-------+------------------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               _c0|             carat|      cut|color|clarity|             depth|             table|            price|                 x|                 y|                 z|\n",
      "+-------+------------------+------------------+---------+-----+-------+------------------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|             53940|             53940|    53940|53940|  53940|             53940|             53940|            53940|             53940|             53940|             53940|\n",
      "|   mean|           26970.5|0.7979397478679852|     NULL| NULL|   NULL| 61.74940489432624| 57.45718390804603|3932.799721913237| 5.731157211716609| 5.734525954764462|3.5387337782723316|\n",
      "| stddev|15571.281096942537|0.4740112444054196|     NULL| NULL|   NULL|1.4326213188336525|2.2344905628213247|3989.439738146397|1.1217607467924915|1.1421346741235616|0.7056988469499883|\n",
      "|    min|                 1|               0.2|     Fair|    D|     I1|                43|                43|             1000|                 0|                 0|                 0|\n",
      "|    max|              9999|              5.01|Very Good|    J|   VVS2|                79|                95|             9999|              9.86|              9.94|              8.06|\n",
      "+-------+------------------+------------------+---------+-----+-------+------------------+------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"diamonds\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|      cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|    Ideal|    E|    SI2| 61.5| 55.0|326.0|3.95|3.98|2.43|\n",
      "| 0.21|  Premium|    E|    SI1| 59.8| 61.0|326.0|3.89|3.84|2.31|\n",
      "| 0.23|     Good|    E|    VS1| 56.9| 65.0|327.0|4.05|4.07|2.31|\n",
      "| 0.29|  Premium|    I|    VS2| 62.4| 58.0|334.0| 4.2|4.23|2.63|\n",
      "| 0.31|     Good|    J|    SI2| 63.3| 58.0|335.0|4.34|4.35|2.75|\n",
      "| 0.24|Very Good|    J|   VVS2| 62.8| 57.0|336.0|3.94|3.96|2.48|\n",
      "| 0.24|Very Good|    I|   VVS1| 62.3| 57.0|336.0|3.95|3.98|2.47|\n",
      "| 0.26|Very Good|    H|    SI1| 61.9| 55.0|337.0|4.07|4.11|2.53|\n",
      "| 0.22|     Fair|    E|    VS2| 65.1| 61.0|337.0|3.87|3.78|2.49|\n",
      "| 0.23|Very Good|    H|    VS1| 59.4| 61.0|338.0| 4.0|4.05|2.39|\n",
      "|  0.3|     Good|    J|    SI1| 64.0| 55.0|339.0|4.25|4.28|2.73|\n",
      "| 0.23|    Ideal|    J|    VS1| 62.8| 56.0|340.0|3.93| 3.9|2.46|\n",
      "| 0.22|  Premium|    F|    SI1| 60.4| 61.0|342.0|3.88|3.84|2.33|\n",
      "| 0.31|    Ideal|    J|    SI2| 62.2| 54.0|344.0|4.35|4.37|2.71|\n",
      "|  0.2|  Premium|    E|    SI2| 60.2| 62.0|345.0|3.79|3.75|2.27|\n",
      "| 0.32|  Premium|    E|     I1| 60.9| 58.0|345.0|4.38|4.42|2.68|\n",
      "|  0.3|    Ideal|    I|    SI2| 62.0| 54.0|348.0|4.31|4.34|2.68|\n",
      "|  0.3|     Good|    J|    SI1| 63.4| 54.0|351.0|4.23|4.29| 2.7|\n",
      "|  0.3|     Good|    J|    SI1| 63.8| 56.0|351.0|4.23|4.26|2.71|\n",
      "|  0.3|Very Good|    J|    SI1| 62.7| 59.0|351.0|4.21|4.27|2.66|\n",
      "+-----+---------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_display = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  CAST(carat AS DOUBLE) AS carat,\n",
    "  CAST(cut AS STRING) AS cut,\n",
    "  CAST(color AS STRING) AS color,\n",
    "  CAST(clarity AS STRING) AS clarity,\n",
    "  CAST(depth AS DOUBLE) AS depth,\n",
    "  CAST(table AS DOUBLE) AS table,\n",
    "  CAST(price AS DOUBLE) AS price,\n",
    "  CAST(x AS DOUBLE) AS x,\n",
    "  CAST(y AS DOUBLE) AS y,\n",
    "  CAST(z AS DOUBLE) AS z\n",
    "FROM diamonds\n",
    "WHERE \n",
    "  carat IS NOT NULL AND\n",
    "  CAST(x AS DOUBLE) != 0 AND\n",
    "  CAST(y AS DOUBLE) != 0 AND\n",
    "  CAST(z AS DOUBLE) != 0\n",
    "\"\"\")\n",
    "\n",
    "df_display.show()\n",
    "\n",
    "df_display.createOrReplaceTempView(\"cleaned_diamonds\")\n",
    "spark.catalog.cacheTable(\"cleaned_diamonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+-----+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|              carat|      cut|color|clarity|             depth|             table|            price|                x|                 y|                 z|\n",
      "+-------+-------------------+---------+-----+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|              53920|    53920|53920|  53920|             53920|             53920|            53920|            53920|             53920|             53920|\n",
      "|   mean| 0.7976982566765384|     NULL| NULL|   NULL|  61.7495140949547|57.456834198813105|3930.993230712166|5.731626854599296| 5.734887054896052|3.5400463649853404|\n",
      "| stddev|0.47379529239063944|     NULL| NULL|   NULL|1.4323310748193272|2.2340642090636664|3987.280445975295|1.119422825791889|1.1401257950841792|0.7025303439301769|\n",
      "|    min|                0.2|     Fair|    D|     I1|              43.0|              43.0|            326.0|             3.73|              3.68|              1.07|\n",
      "|    max|               5.01|Very Good|    J|   VVS2|              79.0|              95.0|          18823.0|            10.74|              58.9|              31.8|\n",
      "+-------+-------------------+---------+-----+-------+------------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"cleaned_diamonds\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check for zero values in x, y, z\n",
    "spark.table(\"cleaned_diamonds\").filter(\"z = 0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_diamonds = spark.sql(\"SELECT * FROM cleaned_diamonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, log, expr\n",
    "\n",
    "engineered_diamonds = (\n",
    "    cleaned_diamonds\n",
    "    # Create volume field as a product of dimensions\n",
    "    .withColumn(\"volume\", col(\"x\") * col(\"y\") * col(\"z\"))\n",
    "\n",
    "    # # Log transform price to help reduce skewness\n",
    "    # .withColumn(\"log_price\", log(col(\"price\")))\n",
    "\n",
    "    # Quality flag for high clarity and color\n",
    "    .withColumn(\"is_high_quality\", when(\n",
    "        (col(\"clarity\").isin(\"IF\", \"VVS1\", \"VVS2\")) & \n",
    "        (col(\"color\").isin(\"D\", \"E\")), 1).otherwise(0))\n",
    ")\n",
    "\n",
    "engineered_diamonds.createOrReplaceTempView(\"engineered_diamonds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Quality Diamonds: 2682\n",
      "Other Diamonds: 51238\n"
     ]
    }
   ],
   "source": [
    "high_quality = engineered_diamonds.filter(\"is_high_quality = 1\").count()\n",
    "low_quality = engineered_diamonds.filter(\"is_high_quality = 0\").count()\n",
    "\n",
    "print(f\"High Quality Diamonds: {high_quality}\")\n",
    "print(f\"Other Diamonds: {low_quality}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-------+-----+-----+------+----+----+----+------------------+---------------+\n",
      "|carat|      cut|color|clarity|depth|table| price|   x|   y|   z|            volume|is_high_quality|\n",
      "+-----+---------+-----+-------+-----+-----+------+----+----+----+------------------+---------------+\n",
      "| 0.24|  Premium|    E|   VVS1| 60.7| 58.0| 553.0|4.01|4.03|2.44|         39.431132|              1|\n",
      "| 0.24|Very Good|    D|   VVS1| 61.5| 60.0| 553.0|3.97| 4.0|2.45|38.906000000000006|              1|\n",
      "| 0.26|Very Good|    E|   VVS2| 59.9| 58.0| 554.0|4.15|4.23|2.51| 44.06179500000001|              1|\n",
      "| 0.26|Very Good|    D|   VVS2| 62.4| 54.0| 554.0|4.08|4.13|2.56|43.137024000000004|              1|\n",
      "| 0.26|Very Good|    D|   VVS2| 62.8| 60.0| 554.0|4.01|4.05|2.53| 41.08846499999999|              1|\n",
      "| 0.26|Very Good|    E|   VVS1| 62.6| 59.0| 554.0|4.06|4.09|2.55|          42.34377|              1|\n",
      "| 0.26|Very Good|    E|   VVS1| 63.4| 59.0| 554.0| 4.0|4.04|2.55|            41.208|              1|\n",
      "| 0.26|Very Good|    D|   VVS1| 62.1| 60.0| 554.0|4.03|4.12|2.53|42.007107999999995|              1|\n",
      "| 0.26|    Ideal|    E|   VVS2| 62.9| 58.0| 554.0|4.02|4.06|2.54|41.455847999999996|              1|\n",
      "| 0.26|     Good|    E|   VVS1| 57.9| 60.0| 554.0|4.22|4.25|2.45|          43.94075|              1|\n",
      "| 0.59|    Ideal|    E|   VVS2| 62.0| 55.0|2761.0|5.38|5.43|3.35| 97.86488999999999|              1|\n",
      "| 0.61|Very Good|    D|   VVS2| 59.6| 57.0|2763.0|5.56|5.58|3.32|103.00233599999999|              1|\n",
      "| 0.63|  Premium|    E|   VVS1| 60.9| 60.0|2765.0|5.52|5.55|3.37|103.24331999999998|              1|\n",
      "| 0.54|    Ideal|    E|   VVS2| 61.6| 56.0|2776.0|5.25|5.27|3.24| 89.64269999999999|              1|\n",
      "| 0.54|    Ideal|    E|   VVS2| 61.5| 57.0|2776.0|5.24|5.26|3.23|         89.026552|              1|\n",
      "| 0.53|Very Good|    D|   VVS2| 57.5| 64.0|2782.0|5.34|5.37|3.08| 88.32146399999999|              1|\n",
      "| 0.63|  Premium|    E|   VVS2| 62.1| 57.0|2789.0|5.48|5.41|3.38|100.20618400000001|              1|\n",
      "| 0.63|  Premium|    E|   VVS1| 60.9| 60.0|2789.0|5.55|5.52|3.37|103.24331999999998|              1|\n",
      "| 0.51|    Ideal|    D|   VVS1| 61.7| 56.0|2797.0|5.12|5.16|3.17|         83.748864|              1|\n",
      "| 0.57|     Fair|    E|   VVS1| 58.7| 66.0|2805.0|5.34|5.43|3.16| 91.62799199999999|              1|\n",
      "+-----+---------+-----+-------+-----+-----+------+----+----+----+------------------+---------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "engineered_diamonds.filter(\"is_high_quality = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_diamonds = engineered_diamonds.drop(\"x\", \"y\", \"z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+-----+-------+------------------+------------------+-----------------+------------------+-------------------+\n",
      "|summary|              carat|      cut|color|clarity|             depth|             table|            price|            volume|    is_high_quality|\n",
      "+-------+-------------------+---------+-----+-------+------------------+------------------+-----------------+------------------+-------------------+\n",
      "|  count|              53920|    53920|53920|  53920|             53920|             53920|            53920|             53920|              53920|\n",
      "|   mean| 0.7976982566765384|     NULL| NULL|   NULL|  61.7495140949547|57.456834198813105|3930.993230712166| 129.8975670619965|0.04974035608308605|\n",
      "| stddev|0.47379529239063944|     NULL| NULL|   NULL|1.4323310748193272|2.2340642090636664|3987.280445975295| 78.21978923213373| 0.2174100496198833|\n",
      "|    min|                0.2|     Fair|    D|     I1|              43.0|              43.0|            326.0|         31.707984|                  0|\n",
      "|    max|               5.01|Very Good|    J|   VVS2|              79.0|              95.0|          18823.0|3840.5980600000003|                  1|\n",
      "+-------+-------------------+---------+-----+-------+------------------+------------------+-----------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "engineered_diamonds.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+------------------+---------------+---------+-----------+-------------+\n",
      "|carat|depth|table|price|            volume|is_high_quality|cut_index|color_index|clarity_index|\n",
      "+-----+-----+-----+-----+------------------+---------------+---------+-----------+-------------+\n",
      "| 0.23| 61.5| 55.0|326.0|          38.20203|              0|      0.0|        1.0|          2.0|\n",
      "| 0.21| 59.8| 61.0|326.0|         34.505856|              0|      1.0|        1.0|          0.0|\n",
      "| 0.23| 56.9| 65.0|327.0|         38.076885|              0|      3.0|        1.0|          3.0|\n",
      "| 0.29| 62.4| 58.0|334.0|          46.72458|              0|      1.0|        5.0|          1.0|\n",
      "| 0.31| 63.3| 58.0|335.0|51.917249999999996|              0|      3.0|        6.0|          2.0|\n",
      "| 0.24| 62.8| 57.0|336.0|38.693951999999996|              0|      2.0|        6.0|          4.0|\n",
      "| 0.24| 62.3| 57.0|336.0|38.830870000000004|              0|      2.0|        5.0|          5.0|\n",
      "| 0.26| 61.9| 55.0|337.0|         42.321081|              0|      2.0|        3.0|          0.0|\n",
      "| 0.22| 65.1| 61.0|337.0|36.425214000000004|              0|      4.0|        1.0|          1.0|\n",
      "| 0.23| 59.4| 61.0|338.0|            38.718|              0|      2.0|        3.0|          3.0|\n",
      "|  0.3| 64.0| 55.0|339.0|           49.6587|              0|      3.0|        6.0|          0.0|\n",
      "| 0.23| 62.8| 56.0|340.0|          37.70442|              0|      0.0|        6.0|          3.0|\n",
      "| 0.22| 60.4| 61.0|342.0|         34.715136|              0|      1.0|        2.0|          0.0|\n",
      "| 0.31| 62.2| 54.0|344.0|51.515744999999995|              0|      0.0|        6.0|          2.0|\n",
      "|  0.2| 60.2| 62.0|345.0|         32.262375|              0|      1.0|        1.0|          2.0|\n",
      "| 0.32| 60.9| 58.0|345.0|51.883728000000005|              0|      1.0|        1.0|          7.0|\n",
      "|  0.3| 62.0| 54.0|348.0|         50.130472|              0|      0.0|        5.0|          2.0|\n",
      "|  0.3| 63.4| 54.0|351.0| 48.99609000000001|              0|      3.0|        6.0|          0.0|\n",
      "|  0.3| 63.8| 56.0|351.0|         48.833658|              0|      3.0|        6.0|          0.0|\n",
      "|  0.3| 62.7| 59.0|351.0|         47.818022|              0|      2.0|        6.0|          0.0|\n",
      "+-----+-----+-----+-----+------------------+---------------+---------+-----------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# VectorAssembler Assembles all of these columns into one single vector. To do this, set the input columns and output column. Then that assembler will be used to transform the prepped data to the final dataset.\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Index categorical features\n",
    "cut_indexer = StringIndexer(inputCol=\"cut\", outputCol=\"cut_index\")\n",
    "color_indexer = StringIndexer(inputCol=\"color\", outputCol=\"color_index\")\n",
    "clarity_indexer = StringIndexer(inputCol=\"clarity\", outputCol=\"clarity_index\")\n",
    "\n",
    "# Transform the DataFrame with the indexers\n",
    "indexed = cut_indexer.fit(engineered_diamonds).transform(engineered_diamonds)\n",
    "indexed = color_indexer.fit(indexed).transform(indexed)\n",
    "indexed = clarity_indexer.fit(indexed).transform(indexed)\n",
    "\n",
    "# Save final DataFrame with indexed columns\n",
    "mapping = indexed\n",
    "\n",
    "indexed = indexed.drop(\"cut\", \"color\", \"clarity\")\n",
    "# indexed = indexed.withColumnRenamed(\"log_price\", \"price\")\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "nonFeatureCols = [\n",
    "    \"price\"\n",
    "]\n",
    "featureCols = [col for col in indexed.columns if col not in nonFeatureCols]\n",
    "\n",
    "assembler = (VectorAssembler()\n",
    "  .setInputCols(featureCols)\n",
    "  .setOutputCol(\"features\"))\n",
    "\n",
    "finalPrep = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37803\n",
      "16117\n"
     ]
    }
   ],
   "source": [
    "training, test = finalPrep.randomSplit([0.7, 0.3])\n",
    "\n",
    "#  Going to cache the data to make sure things stay snappy!\n",
    "training.cache()\n",
    "test.cache()\n",
    "\n",
    "print(training.count()) # Why execute count here??\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "rfModel = (RandomForestRegressor()\n",
    "  .setLabelCol(\"price\")\n",
    "  .setFeaturesCol(\"features\"))\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "  .addGrid(rfModel.maxDepth, [5, 10])\n",
    "  .addGrid(rfModel.numTrees, [20, 60])\n",
    "  .build())\n",
    "# Note, that this parameter grid will take a long time\n",
    "# to run in the community edition due to limited number\n",
    "# of workers available! Be patient for it to run!\n",
    "# If you want it to run faster, remove some of\n",
    "# the above parameters and it'll speed right up!\n",
    "\n",
    "stages = [rfModel]\n",
    "\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "\n",
    "cv = (CrossValidator() # you can feel free to change the number of folds used in cross validation as well\n",
    "  .setEstimator(pipeline) # the estimator can also just be an individual model rather than a pipeline\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setEvaluator(RegressionEvaluator().setLabelCol(\"price\")))\n",
    "\n",
    "pipelineFitted = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Parameters:\n",
      "--------------------\n",
      "RandomForestRegressionModel: uid=RandomForestRegressor_c262131eff68, numTrees=20, numFeatures=8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestRegressor_c262131eff68', name='bootstrap', doc='Whether bootstrap samples are used when building trees.'): True,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'auto',\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance',\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='labelCol', doc='label column name.'): 'price',\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='numTrees', doc='Number of trees to train (>= 1).'): 20,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='seed', doc='random seed.'): 6764255577172757275,\n",
       " Param(parent='RandomForestRegressor_c262131eff68', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The Best Parameters:\\n--------------------\")\n",
    "print(pipelineFitted.bestModel.stages[0])\n",
    "pipelineFitted.bestModel.stages[0].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_f7feee4cc687"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelineFitted.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+-----+---------+-------+------------+\n",
      "|    raw_prediction|prediction|price|cut_index|pct_off|pct_accuracy|\n",
      "+------------------+----------+-----+---------+-------+------------+\n",
      "|509.89059955622287|     510.0|367.0|      0.0|  38.93|       61.07|\n",
      "| 552.4290700127754|     552.0|367.0|      0.0|  50.53|       49.47|\n",
      "| 542.0659199708288|     542.0|404.0|      1.0|  34.17|       65.83|\n",
      "| 516.0763758267157|     516.0|470.0|      1.0|    9.8|        90.2|\n",
      "|469.13064742024716|     469.0|327.0|      3.0|  43.47|       56.53|\n",
      "| 563.3670676379622|     563.0|438.0|      2.0|  28.62|       71.38|\n",
      "| 644.6659498107296|     645.0|550.0|      1.0|  17.21|       82.79|\n",
      "| 549.3501843743887|     549.0|530.0|      2.0|   3.65|       96.35|\n",
      "| 464.4631461342551|     464.0|373.0|      2.0|  24.52|       75.48|\n",
      "| 483.4722606040126|     483.0|402.0|      2.0|  20.27|       79.73|\n",
      "| 568.6253397117944|     569.0|530.0|      2.0|   7.29|       92.71|\n",
      "| 570.3690150620312|     570.0|505.0|      2.0|  12.94|       87.06|\n",
      "| 645.1962082360244|     645.0|640.0|      1.0|   0.81|       99.19|\n",
      "| 563.9708662940978|     564.0|458.0|      2.0|  23.14|       76.86|\n",
      "| 588.2152798785883|     588.0|682.0|      2.0|  13.75|       86.25|\n",
      "| 501.1067615789395|     501.0|402.0|      2.0|  24.65|       75.35|\n",
      "| 578.4237637791688|     578.0|465.0|      2.0|  24.39|       75.61|\n",
      "| 559.2594320722162|     559.0|478.0|      2.0|   17.0|        83.0|\n",
      "| 578.8354406584237|     579.0|530.0|      2.0|   9.21|       90.79|\n",
      "| 572.5413565645667|     573.0|472.0|      2.0|   21.3|        78.7|\n",
      "+------------------+----------+-----+---------+-------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "holdout2 = (pipelineFitted.bestModel\n",
    "  .transform(test)\n",
    "  .selectExpr(\"prediction as raw_prediction\", \n",
    "              \"double(round(prediction)) as prediction\",\n",
    "              \"price \", \n",
    "              \"cut_index\",\n",
    "              \"round(abs((prediction - price) / price) * 100, 2) AS pct_off\",\n",
    "              \"round(100 - (abs((prediction - price) / price) * 100), 2) AS pct_accuracy\"\n",
    "  ))\n",
    "  \n",
    "holdout2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.9720007234928092\n",
      "MAE: 352.8628793743668\n",
      "RMSE: 665.9413324221837\n",
      "R^2: 0.9720007234928092\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate Mean Squared Error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", \n",
    "    predictionCol=\"raw_prediction\", \n",
    "    metricName=\"r2\"\n",
    ")\n",
    "mse = evaluator.evaluate(holdout2)\n",
    "\n",
    "mae = evaluator.setMetricName(\"mae\").evaluate(holdout2)\n",
    "rmse = evaluator.setMetricName(\"rmse\").evaluate(holdout2)\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(holdout2)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 9.95%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, abs\n",
    "\n",
    "# Mean Absolute Percentage Error\n",
    "mape_df = holdout2.withColumn(\n",
    "    \"pct_error\", abs(col(\"raw_prediction\") - col(\"price\")) / col(\"price\")\n",
    ")\n",
    "\n",
    "mape = mape_df.selectExpr(\"avg(pct_error)\").first()[0]\n",
    "print(f\"MAPE: {mape * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|avg_prediction_accuracy|\n",
      "+-----------------------+\n",
      "|      90.04846125209383|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "holdout2.selectExpr(\"avg(pct_accuracy) as avg_prediction_accuracy\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|      cut|     avg_accuracy|\n",
      "+---------+-----------------+\n",
      "|  Premium|90.00256478566219|\n",
      "|    Ideal|89.89090993214067|\n",
      "|     Good|89.83024561403501|\n",
      "|     Fair|84.95818763326223|\n",
      "|Very Good|91.13138781163435|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "cut_mapping = mapping.select(\"cut\", \"cut_index\").distinct()\n",
    "holdout2_with_cut = holdout2.join(cut_mapping, on=\"cut_index\", how=\"left\")\n",
    "\n",
    "# Group by cut index and calculate average accuracy\n",
    "holdout2_with_cut.groupBy(\"cut\").agg(avg(\"pct_accuracy\").alias(\"avg_accuracy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop("
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment_4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
